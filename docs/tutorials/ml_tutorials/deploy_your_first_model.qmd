---
title: 'Deploy your first model'
---

This tutorial guides you through deploying a trained model as a prediction endpoint. You'll learn how to serve models via Apache Arrow Flight for low-latency predictions.

By the end, you'll have a running model server that accepts prediction requests.

## Why use Flight for model serving?

Apache Arrow Flight is a high-performance RPC framework designed for data transfer. For ML serving, Flight offers:

- **Fast data transfer:** Zero-copy data exchange using Apache Arrow
- **Schema validation:** Automatic type checking for inputs and outputs
- **Streaming:** Handle large batches efficiently
- **Language support:** Client libraries for Python, R, Java, and more

Think of Flight as a production-ready way to expose your model as an API.

:::{.callout-tip}
### Arrow's zero-copy advantage
Flight uses Apache Arrow's columnar format, which means no serialization overhead. Data goes from client to server to model without conversion—this is much faster than JSON or pickle-based APIs.
:::

## Prepare a model for deployment

Let's train a simple model that we'll deploy:

```{python}
import xorq.api as xo
from sklearn.feature_extraction.text import TfidfVectorizer
from xorq.expr.ml import deferred_fit_transform_series_sklearn
import xorq.vendor.ibis.expr.datatypes as dt
from xorq.caching import ParquetStorage
import pandas as pd

# <1>
con = xo.connect()
storage = ParquetStorage(source=con)

# <2>
sample_data = pd.DataFrame({
    "id": [1, 2, 3, 4, 5],
    "title": [
        "Machine learning tutorial",
        "Python data science",
        "Apache Arrow performance",
        "Database optimization tips",
        "Xorq ML deployment"
    ]
})
train_table = con.register(sample_data, "train")

# <3>
deferred_tfidf = deferred_fit_transform_series_sklearn(
    col="title",
    cls=TfidfVectorizer,
    return_type=dt.Array(dt.float64)
)

# <4>
(model_expr, model_udaf, transform_func) = deferred_tfidf(
    train_table,
    storage=storage
)

print("Model ready for deployment")
print(f"Training data: {train_table.count().execute()} rows")
```
1. Set up connection and storage for caching.
2. Create sample text data for training.
3. Create a deferred TF-IDF transformer.
4. Fit the model with caching—this trains and caches the TF-IDF vectorizer.

Now we have a trained model that can transform text into TF-IDF vectors.

## Create a prediction expression

Before deploying, create an expression that the server will execute:

```{python}
# <1>
test_data = pd.DataFrame({
    "id": [100, 101],
    "title": ["New machine learning post", "Database tutorial"]
})
test_table = con.register(test_data, "test")

# <2>
predictions = test_table.mutate(
    tfidf=transform_func.on_expr(test_table)
)

# <3>
print("Prediction expression created (not executed yet)")
print(f"Input columns: {test_table.columns}")
print(f"Output columns: {predictions.columns}")
```
1. Create test data for predictions.
2. Build a prediction expression that adds TF-IDF features.
3. The expression is ready but not executed.

This expression will be served by the Flight server.

:::{.callout-note}
### Serving expressions
Flight servers serve expressions, not just models. This means you can deploy entire data transformation + prediction pipelines, not just the final prediction step.
:::

## Serve the model with Flight

Now let's deploy the model using Xorq's Flight serving:

```{python}
# <1>
server, do_exchange = xo.expr.relations.flight_serve(predictions)

# <2>
print(f"Flight server started!")
print(f"Server location: {server.location}")
print(f"Ready to accept prediction requests")
```
1. Start a Flight server that serves the predictions expression.
2. The server is now running and ready for requests.

The `flight_serve()` function returns:
- `server`: The running Flight server
- `do_exchange`: A function to send data and get predictions

:::{.callout-warning}
### Server lifecycle
The server runs until you explicitly close it with `server.close()`. In production, you'd run the server as a long-lived process, not in a notebook cell.
:::

## Make predictions via the client

Let's use the client function to make predictions:

```{python}
# <1>
client_data = pd.DataFrame({
    "id": [200, 201, 202],
    "title": [
        "Apache Arrow Flight tutorial",
        "Xorq machine learning guide",
        "Optimizing data pipelines"
    ]
})
client_table = con.register(client_data, "client_input")

# <2>
result = do_exchange(client_table)

# <3>
predictions_df = result.read_pandas()

# <4>
print("Predictions received:")
print(predictions_df[["id", "title"]])
print(f"\nTF-IDF vector shape for first row: {len(predictions_df['tfidf'][0])}")

# <5>
server.close()
print("\nServer closed")
```
1. Create new data for prediction (client-side).
2. Send data to the server and request predictions.
3. Read the response as a pandas DataFrame.
4. View the predictions—each row now has a TF-IDF vector.
5. Close the server when done.

The client sends data to the server, the server runs the prediction expression, and returns results—all using efficient Arrow format.

## Handle schema validation

Flight automatically validates that client data matches the expected schema:

```{python}
# <1>
server2, do_exchange2 = xo.expr.relations.flight_serve(predictions)

# <2>
try:
    wrong_schema = pd.DataFrame({
        "id": [1, 2],
        "wrong_column": ["text1", "text2"]  # Missing "title" column
    })
    wrong_table = con.register(wrong_schema, "wrong")
    result = do_exchange2(wrong_table)
    print("This shouldn't print")
except Exception as e:
    print(f"Schema validation error: {type(e).__name__}")
    print("Server rejected data with wrong schema!")

# <3>
server2.close()
```
1. Start a new server.
2. Try to send data with the wrong schema.
3. Flight rejects the request because the schema doesn't match.

This automatic validation prevents runtime errors from bad input data.

:::{.callout-tip}
### Schema as a contract
Flight uses Arrow schemas as an API contract. The client must send data matching the expected schema, or the request fails. This catches errors early instead of during prediction.
:::

## Production serving pattern

For production deployments, you'd typically:

1. Train and cache your model
2. Create a prediction expression
3. Start a Flight server in a dedicated process
4. Clients connect and send prediction requests
5. Server runs predictions and returns results

Here's a pattern for production serving:

```python
# server.py - Run this as a long-lived process
import xorq.api as xo
from xorq.caching import ParquetStorage
from xorq.expr.ml import deferred_fit_transform_series_sklearn
from sklearn.feature_extraction.text import TfidfVectorizer
import xorq.vendor.ibis.expr.datatypes as dt

# Setup
con = xo.connect()
storage = ParquetStorage(source=con)

# Load training data and fit model
train_table = xo.read_parquet("s3://bucket/train.parquet")
deferred_tfidf = deferred_fit_transform_series_sklearn(
    col="title",
    cls=TfidfVectorizer,
    return_type=dt.Array(dt.float64)
)
(_, _, transform_func) = deferred_tfidf(train_table, storage=storage)

# Create prediction template
# Note: we don't execute yet—server will execute per request
prediction_template = con.register(
    your_input_schema_table, "input"
).mutate(tfidf=transform_func.on_expr)

# Start server
server, do_exchange = xo.expr.relations.flight_serve(prediction_template)
print(f"Server running at {server.location}")

# Keep server alive
try:
    while True:
        time.sleep(1)
except KeyboardInterrupt:
    server.close()
    print("Server stopped")
```

## Client usage

Clients connect to the Flight server to get predictions:

```python
# client.py - Clients run this to get predictions
import xorq.api as xo
import pandas as pd

# Prepare data
new_data = pd.DataFrame({
    "id": [1, 2, 3],
    "title": ["New article 1", "New article 2", "New article 3"]
})

# Connect and get predictions
con = xo.connect()
input_table = con.register(new_data, "input")

# do_exchange() is the client function returned by flight_serve
# In production, you'd get this from the server connection
result = do_exchange(input_table)
predictions = result.read_pandas()

print(predictions[["id", "title", "tfidf"]])
```

:::{.callout-note}
### Batch predictions
Flight is designed for batch predictions. Send 100 or 10,000 rows at once—Arrow's columnar format handles batches efficiently.
:::

## Compare: Flight vs REST API

Here's why Flight is better than REST for ML serving:

| Aspect | Flight | REST API |
|--------|--------|----------|
| Data format | Arrow (columnar) | JSON (row-based) |
| Serialization | Zero-copy | Parse/stringify overhead |
| Type safety | Schema validated | Manual validation needed |
| Batch size | Efficient for large batches | Slow for large payloads |
| Latency | <10ms for small batches | 50-100ms typical |

Flight is built for data-intensive workloads, making it ideal for ML serving.

## Complete example

Here's a full deployment workflow:

```python
import xorq.api as xo
from sklearn.neighbors import KNeighborsClassifier
from xorq.expr.ml import Step
from xorq.caching import ParquetStorage
import pandas as pd

# Setup
con = xo.connect()
storage = ParquetStorage(source=con)

# Train model
iris = xo.examples.iris.fetch(backend=con)
target = "species"
features = tuple(iris.drop(target).columns)

model = Step(typ=KNeighborsClassifier, n_neighbors=5)
fitted_model = model.fit(
    expr=iris,
    features=features,
    target=target,
    storage=storage
)

# Create prediction expression
prediction_expr = iris.mutate(
    predicted=fitted_model.predict_raw(iris, name="predicted")
)

# Deploy
server, do_exchange = xo.expr.relations.flight_serve(prediction_expr)
print(f"Model deployed at {server.location}")

# Make predictions
new_data = pd.DataFrame({
    "sepal_length": [5.1, 6.3],
    "sepal_width": [3.5, 2.9],
    "petal_length": [1.4, 5.6],
    "petal_width": [0.2, 1.8],
    "species": ["", ""]  # Required in schema but not used
})
new_table = con.register(new_data, "new")
result = do_exchange(new_table)
predictions = result.read_pandas()

print(predictions[["sepal_length", "predicted"]])

# Clean up
server.close()
```

## Next steps

You've deployed your first model! Continue exploring:

- [Understand Flight protocol](../ai_tutorials/understand_flight_protocol.qmd) dives deeper into Flight architecture
- [Explore model evaluation](explore_model_evaluation.qmd) shows how to select the best model to deploy
- Check the Guides section for production deployment patterns
