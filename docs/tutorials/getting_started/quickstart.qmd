---
title: "Quickstart"
description: "Build and run your first Xorq ML pipeline in under 10 minutes"
icon: "pencil-square"
headline: "Get a first taste of Xorq"
---

After completing this guide, you will have a working ML pipeline that loads data, trains a model, and generates predictions—all using Xorq's deferred execution model.

## What you'll build

In this quickstart, you will:

1. Install Xorq and initialize a project
2. Build a pipeline expression
3. Run your pipeline and save results
4. Serve your pipeline as an endpoint

The entire process takes about 10 minutes. By the end, you'll understand how Xorq transforms Python code into executable, servable pipelines.

---

## Step 1: Install Xorq

Install Xorq using your preferred package manager.

::: {.panel-tabset}

### pip (macOS/Linux)

```bash
pip install xorq[examples]
```

### pip (Windows)

```bash
pip install "xorq[examples]"
```

### nix

```bash
nix run github:xorq-labs/xorq
```

:::

::: {.callout-tip}
## Verify your installation
Open a Python shell and check the version:

```python
python
>>> import xorq
>>> xorq.__version__
'0.3.3'
>>> exit() or Ctrl-D 
```

If you see a version number, then Xorq is installed correctly.
:::

With Xorq installed, the next step is to create your first project.

---

## Step 2: Initialize a project

Create a new Xorq project using the built-in penguins template. This template demonstrates a complete ML workflow with the Palmer Penguins dataset.

```bash
xorq init -t penguins -p penguins_example # <1>
cd penguins_example # <2>
```
1. Creates a new project called `penguins_example` using the penguins template
2. Moves into the project directory

The template generates an `expr.py` file containing a complete ML pipeline with data loading, train/test splitting, model training, and prediction.

::: {.callout-note}
## What's in the template?
The penguins template includes caching, lineage tracking, and a scikit-learn logistic regression model. You can explore the code in `expr.py` after completing this quickstart.
:::

With your project initialized, the next step is to build it into an executable format.

---

## Step 3: Build your expression

Convert your pipeline into a serialized, executable format using the `build` command.

```bash
xorq build expr.py
```

**Output:**

```
Building expr from expr.py
Written 'expr' to builds/12287e173c17
builds/12287e173c17
```

The build process creates a directory (for example, `builds/12287e173c17`) containing your serialized pipeline. This hash uniquely identifies your build.

<!-- SCREENSHOT: Show the builds directory structure with manifest.json visible -->

With your pipeline built, the next step is to run it and see results.

---

## Step 4: Run your pipeline

Execute your built pipeline and view the results.

::: {.callout-warning}
## Windows users
Skip the first command and go straight to saving results to a file. Running without an output file may cause issues on Windows.
:::

::: {.panel-tabset}

### View results

```bash
xorq run builds/12287e173c17
```

### Save to file

```bash
xorq run builds/12287e173c17 -o predictions.parquet
```

### Limit output

Return only a subset of rows to quickly verify the pipeline works.

```bash
xorq run builds/12287e173c17 --limit 10
```

:::

Replace `12287e173c17` with the hash from your own build output.

You should see prediction results from your ML pipeline. If you saved to a file, then you can open `predictions.parquet` with any Parquet-compatible tool.

<!-- SCREENSHOT: Show terminal output with prediction results -->

With your pipeline running locally, the next step is to serve it as an endpoint.

---

## Step 5: Serve your pipeline

Serve your pipeline as an API endpoint using Arrow Flight. This allows other applications to query your pipeline over the network.

First, start the Flight server with your built pipeline:

```bash
xorq serve-flight-udxf --port 8001 builds/12287e173c17
```

Then, in a new terminal, serve your specific pipeline node:

```bash
xorq serve-unbound builds/12287e173c17 --host localhost --port 8001 --to_unbind_hash da8ba93cc97709f4ad9502e16d71e8e3
```

::: {.callout-important}
## About the node hash
The hash `da8ba93cc97709f4ad9502e16d71e8e3` is specific to this quickstart template. When you build your own pipelines, you will generate different hashes. For this tutorial, use the exact hash shown above.
:::

Your pipeline is now running as an endpoint on `localhost:8001`.

<!-- SCREENSHOT: Show terminal output confirming the server is running -->

---

## Query your served pipeline

With the server running, you can query your pipeline from Python.

```{python}
#| eval: false
import xorq.api as xo

client = xo.flight.client.FlightClient(port=8001) # <1>

data_url = "https://storage.googleapis.com/letsql-pins/penguins/20250703T145709Z-c3cde/penguins.parquet"

expr = (
    xo.deferred_read_parquet(
        con=xo.duckdb.connect(),
        path=data_url,
        table_name="penguins",
    )
    .select("bill_length_mm", "bill_depth_mm", "species") # <2>
    .drop_null()
    .limit(5)
)

fut, rbr = client.do_exchange("default", expr) # <3>
result_df = rbr.read_pandas() # <4>
print(result_df)
```
1. Connect to the Flight server on port 8001
2. Select only the columns needed for prediction
3. Execute the expression via Flight's do_exchange protocol
4. Convert the result to a pandas DataFrame

You should see a DataFrame with penguin measurements and species predictions.

---

## What you've accomplished

You've just built your first Xorq ML pipeline. Here's what happened:

- **Initialized** a project with a complete ML workflow
- **Built** the pipeline into a serialized, portable format
- **Ran** the pipeline to generate predictions
- **Served** the pipeline as an API endpoint
- **Queried** the endpoint from Python

The entire workflow used Xorq's deferred execution model. No computation happened until you explicitly ran or queried the pipeline.

---

## Next steps

::: {.grid}

::: {.g-col-6}
### Explore the code
Open `expr.py` in your project to see how the pipeline is constructed. Modify the model parameters and rebuild to see changes.

[Understand deferred execution →](/concepts/deferred-execution)
:::

::: {.g-col-6}
### Learn about caching
The penguins template uses `ParquetStorage` for caching. Learn how caching speeds up repeated pipeline runs.

[Caching guide →](/concepts/caching)
:::

::: {.g-col-6}
### Configure backends
Xorq supports multiple execution backends including DuckDB and DataFusion. Learn how to switch between them.

[Backend configuration →](/reference/backend/profiles)
:::

::: {.g-col-6}
### Join the community
Get help, share your projects, and connect with other Xorq users.

[Discord community →](https://discord.gg/8Kma9DhcJG)
:::

:::
