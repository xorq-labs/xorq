---
title: "Quickstart"
description: "Build and run your first Xorq ML pipeline in a few minutes"
icon: "pencil-square"
headline: "Get a first taste of Xorq"
---

After completing this guide, you have a working ML pipeline. This pipeline loads data, trains a model, and generates predictions. It uses Xorq's deferred execution model.

## What you'll build

In this quickstart, you:

1. Set up your environment and install Xorq
2. Initialize a project
3. Build a pipeline expression
4. Run your pipeline and save results
5. Deploy your pipeline as an API endpoint
6. Serve a UDXF expression as an endpoint

The entire process takes a few minutes. By the end, you understand how Xorq transforms Python code into executable, servable pipelines.

## Step 1: Set up your environment and install Xorq

This step covers environment setup, Xorq installation, and verification.

### Check your Python version

::: {.callout-tip}
Xorq requires Python 3.10 or higher, but also no higher than 3.13. Check your version with `python --version`. If you need to install or update Python, then visit the [official Python downloads page](https://www.python.org/downloads/).
:::

### Create a virtual environment

Create and activate a virtual environment:

::: {.panel-tabset}

### macOS/Linux
```bash
python -m venv .venv
source .venv/bin/activate
```

### Windows
```bash
python -m venv .venv
.venv\Scripts\activate
```

:::

### Update pip

Before installing Xorq, update pip to the latest version. This avoids compatibility issues:
```bash
python -m pip install --upgrade pip
```

### Install Xorq

Now install Xorq using your preferred package manager:

::: {.panel-tabset}

### pip (macOS/Linux/Windows)
```bash
pip install "xorq[examples]"
```

### nix
```bash
nix run github:xorq-labs/xorq
```

:::

### Verify installation

Verify your installation by checking the version. Open a Python shell:
```bash
python
```

::: {.callout-note}
If `python` does not work, then try `python3` instead.
:::

Import Xorq and check the version:
```python
import xorq
xorq.__version__
```

You see a version number like `'0.3.7'`.

Exit the Python shell:
```python
exit()
```

If you see a version number, then Xorq is installed correctly. To work through a full pipeline, you need a project.

## Step 2: Initialize a project

Create a new Xorq project using the built-in penguins template. This template demonstrates a complete ML workflow with the Palmer Penguins dataset:
```bash
xorq init -t penguins -p penguins_example # <1>
cd penguins_example # <2>
```
1. Creates a new project called penguins_example using the penguins template.
2. Moves into the project directory.

::: {.callout-note}
If `xorq init` fails with `[SSL: CERTIFICATE_VERIFY_FAILED]` or "unable to get local issuer certificate", then Python cannot verify the download. Set the certificate bundle, then retry:
```bash
export SSL_CERT_FILE=$(python -m certifi)
xorq init -t penguins -p penguins_example
```
See [Installation issues](troubleshooting/installation_issues.qmd#issue-ssl-certificate-verify-failed-on-macos) for details and a permanent fix.
:::

The template generates an `expr.py` file with a complete ML pipeline: data loading, train/test splitting, model training, and prediction. That file is plain Python; to run it in Xorq's portable format, you build it.

## Step 3: Build your expression

Convert your pipeline into a serialized, executable format using the `build` command:
```bash
xorq build expr.py
```

This serializes your pipeline and generates a content-addressed build directory.

You'll see output like:

```
Building expr from expr.py
Written 'expr' to builds/12287e173c17
builds/12287e173c17
```

The build creates a directory like `builds/12287e173c17` containing your serialized pipeline. This hash uniquely identifies your build.

![](../images/quickstart/build-output-hash.png)

Save this hash as an environment variable:

::: {.panel-tabset}

### macOS/Linux
```bash
export BUILD_HASH=12287e173c17
```

Replace `12287e173c17` with your actual hash.



### Windows (CMD)
```cmd
set BUILD_HASH=12287e173c17
```

Replace `12287e173c17` with your actual hash.

:::

The build directory is content-addressed: the hash identifies this exact pipeline. Running it executes the pipeline and produces results.

## Step 4: Run your pipeline

Execute your built pipeline and view the results. Replace `<BUILD_HASH>` with the hash from your build output in step 3:

::: {.callout-note}
Use the "Save to file" option to save results to a file. Running without an output file discards results.
:::

::: {.panel-tabset}

### Save to file (macOS/Linux)
```bash
xorq run builds/$BUILD_HASH -o predictions.parquet
```

Runs the pipeline and saves results to `predictions.parquet`. You can open this file with pandas, DuckDB, or Polars.

### Save to file (Windows)
```cmd
xorq run builds/%BUILD_HASH% -o predictions.parquet
```

Runs the pipeline and saves results to `predictions.parquet`. You can open this file with pandas, DuckDB, or Polars.

### View results in terminal (macOS/Linux)
```bash
xorq run builds/$BUILD_HASH -o - --format csv
```

Outputs results as CSV to your terminal. You can also view results as JSON by using `--format json`.

:::

So far, the pipeline has run only on your machine; to accept input from clients over the network, you deploy it as an API.

## Step 5: Deploy your pipeline as an API endpoint

::: {.callout-important}
This step uses the same `BUILD_HASH` from step 3. You're deploying the exact pipeline you just built and ran.
:::

### Start the server

`serve-unbound` runs your pipeline as an API and replaces one node (the cached training data) so clients can send their own data. For the penguins template, use this hash:

::: {.panel-tabset}

### macOS/Linux
```bash
xorq serve-unbound builds/$BUILD_HASH \
  --host localhost \
  --port 8001 \
  --to_unbind_hash 0eb004b30d161217f4cba885038c5c02
```

### Windows
```cmd
xorq serve-unbound builds/%BUILD_HASH% --host localhost --port 8001 --to_unbind_hash 0eb004b30d161217f4cba885038c5c02
```

:::

::: {.callout-note}
If you get an error like "not enough values to unpack", then the template or Xorq version may have changed. Open `builds/<BUILD_HASH>/expr.yaml`, find the `CachedNode` with `name: xorq_cached_node_name_placeholder`, and use its full `snapshot_hash` value instead.
:::

You'll see output like:

```
2026-02-04T12:41:46.350082Z [info     ] Loading expression from builds/bfd4918da5cd
2026-02-04T12:41:46.350638Z [info     ] console metrics enabled, interval=2000 ms
2026-02-04T12:41:58.274157Z [info     ] Serving expression from 'builds/bfd4918da5cd' on grpc://localhost:8001
```

The server is listening on `grpc://localhost:8001`. When you're ready to add a UDXF endpoint, stop this server with Ctrl+C in the terminal where it's running so port 8001 is free.

## Step 6: Serve a UDXF expression

::: {.callout-note}
Open a new terminal window. Activate the same virtual environment (`.venv`) and navigate to the same directory (`penguins_example`). Ensure the pipeline server from Step 5 is stopped so port 8001 is free.
:::

Create a file called `udxf_example.py`:

```python
# udxf_example.py
import pandas as pd
import xorq.api as xo

# <1>
def add_computed_column(df: pd.DataFrame) -> pd.DataFrame:
    """Add a computed column that doubles the input value."""
    result = df.copy()
    result["doubled"] = result["value"] * 2
    return result

# <2>
input_schema = xo.schema({"value": "int64"})
output_schema = xo.schema({"value": "int64", "doubled": "int64"})

# <3>
con = xo.connect()
input_table = xo.memtable({"value": [1, 2, 3, 4, 5]}, schema=input_schema)

# <4>
expr = xo.expr.relations.flight_udxf(
    input_table,
    process_df=add_computed_column,
    maybe_schema_in=input_schema,
    maybe_schema_out=output_schema,
    con=con,
    make_udxf_kwargs={
        "name": "double_value",
        "command": "double_value"
    }
)
```
1. Defines a transformation function that doubles input values.
2. Specifies input and output schemas for type safety.
3. Creates a simple input table with test data.
4. Creates the UDXF expression with the transformation function.

### Build the UDXF expression

Build the expression:

```bash
xorq build udxf_example.py --expr-name expr
```

This creates a second `<HASH>` in your `builds/<HASH>/` directory with your serialized UDXF expression. 

![](../images/quickstart/second-hash.png)

### Start the Flight server

Start the Flight server with your built UDXF expression. Replace `<BUILD_HASH>` with the second hash from your build output:

```bash
xorq serve-flight-udxf --port 8001 builds/<BUILD_HASH>
```

Your UDXF is now running as an endpoint on `localhost:8001`. Keep this terminal window open while you query it.

### Query your served UDXF

With the server running, create a new Python file called `query_udxf.py`:

```python
# query_udxf.py
import xorq.api as xo
import random as rnd

# <1>
con = xo.flight.connect(port=8001)

# <2>
exchange = con.get_exchange("default")

# <3>
expr = xo.memtable({ "value" : rnd.choices(tuple(range(100)), k=10) }, schema= xo.schema({"value": "int64"})).pipe(exchange)

# <4>
print("Executing via Flight do_exchange...")
result_df = expr.execute()

print(result_df)
```
1. Connect to the Flight server running on port 8001. This establishes a connection to communicate with your deployed server.
2. Get the exchange named "default" from the connection. This exchange represents the UDXF transformation you deployed.
3. Create an in-memory table with random integer values and pipe it through the exchange. This builds an expression that will apply your UDXF transformation to the input data.
4. Execute the expression via Flight `do_exchange` and print the results. The server applies the transformation (doubling the values) and returns the result as a pandas DataFrame.

### Run the query script

Run your query script:

```bash
python query_udxf.py
```

You'll see output like:

```
Executing via Flight do_exchange...
   value  doubled
0     89      178
1     27       54
2     85      170
...
```

The server applied your UDXF transformation (doubling the values) and returned the result via the Flight protocol.

## What you built

You built your first Xorq ML pipeline: initialized a project, built it into a portable format, ran it to generate predictions, deployed it as an API, and served a UDXF expression as an endpoint.

The entire workflow uses Xorq's deferred execution model. No computation runs until you explicitly execute or query the pipeline.

## Next steps

Continue learning:

- [Install Xorq](installation.qmd) — set up Xorq and optional backends.
- [Defer query execution](understand_deferred_execution.qmd) — how Xorq builds computation graphs before executing them.
- [Cache expression results](explore_caching.qmd) — how Xorq caches results to speed up workflows.
- [Switch between backends](switch_backends.qmd) — run the same pipeline on DuckDB, Snowflake, and other engines.
- [Join the community](https://discord.gg/8Kma9DhcJG) on Discord to get help and share your projects.