---
title: "Quickstart"
icon: "pencil-square"
headline: "Get a first taste of xorq"
---

## Installation

Xorq can be installed using pip:

```bash
pip install xorq
```

Or using nix to drop into an IPython shell:

```bash
nix run github:xorq-labs/xorq
```

### Initialize a Project

The fastest way to get started with Xorq is to use the `xorq init` command:

```bash
xorq init -t penguins
```

This creates a starting example project with the Palmer Penguins dataset that demonstrates key xorq features including machine learning pipelines, caching, and lineage tracking.

Navigate to the created directory and explore the `expr.py` file to see how xorq works in practice.

## Understanding the Generated Pipeline

The template creates an `expr.py` file that demonstrates a complete ML workflow. Let's walk through the key components:

### 1. Data Loading and Preparation

```{python}
# https://inria.github.io/scikit-learn-mooc/python_scripts/trees_classification.html
import sklearn
from sklearn.linear_model import LogisticRegression

import xorq as xo
from xorq.caching import ParquetStorage
from xorq.expr.ml.pipeline_lib import (
    Pipeline,
)


features = ("bill_length_mm", "bill_depth_mm")
target = "species"
data_url = "https://storage.googleapis.com/letsql-pins/penguins/20250703T145709Z-c3cde/penguins.parquet"

```

### 2. Create Train/Test Splits

The template includes a utility function for creating reproducible data splits:

```{python}

def gen_splits(expr, test_size=.2, random_seed=42, **split_kwargs):
    # inject and drop row number
    assert "test_sizes" not in split_kwargs
    assert isinstance(test_size, float)
    row_number = "row_number"
    yield from (
        expr.drop(row_number)
        for expr in xo.train_test_splits(
            expr.mutate(**{row_number: xo.row_number()}),
            unique_key=row_number,
            test_sizes=test_size,
            random_seed=random_seed,
            **split_kwargs,
        )
    )


def get_penguins_splits(storage=None, **split_kwargs):
    t = (
        xo.deferred_read_parquet(
            con=xo.duckdb.connect(),
            path=data_url,
            table_name="t",
        )
        .select(features+(target,))
        .drop_null()
    )
    (train, test) = (
        expr
        .cache(storage or ParquetStorage())
        for expr in gen_splits(t, **split_kwargs)
    )
    return (train, test)
```

### 3. Define and Configure Pipeline

```{python}
def make_pipeline(params=()):
    clf = (
        sklearn.pipeline.Pipeline(
            steps=[
                ("logistic", LogisticRegression()),
            ]
        )
        .set_params(**dict(params))
    )
    return clf


def fit_and_score_sklearn_pipeline(pipeline, train, test):
    (
        (X_train, y_train),
        (X_test, y_test),
    ) = (
        expr.execute().pipe(lambda t: (
            t.filter(regex=f"^(?!{target})"),
            t.filter(regex=f"^{target}"),
        ))
        for expr in (train, test)
    )
    clf = pipeline.fit(X_train, y_train)
    score = clf.score(X_test, y_test)
    return clf, score
```

### 4. Create Deferred ML Pipeline

The key innovation is converting scikit-learn pipelines to deferred xorq expressions:

```{python}
# Configure hyperparameters
params = {
    "logistic__C": 1E-4,
}


# Get train/test splits
(train, test) = get_penguins_splits()

# Create scikit-learn pipeline
sklearn_pipeline = make_pipeline(params=params)

# Convert to xorq pipeline
xorq_pipeline = Pipeline.from_instance(sklearn_pipeline)

# still no work done: deferred fit expression
fitted_pipeline = xorq_pipeline.fit(train, features=features, target=target)

train_predicted = fitted_pipeline.fitted_steps[-1].predicted
expr = test_predicted = fitted_pipeline.predict(test[features])
```

The beauty here is that `fitted_pipeline` is still just an expression - no actual training has happened yet. The computation is deferred until you call `.execute()`.

### 5. Execute and Validate

```{python}

# Compare xorq vs sklearn results
clf, score_sklearn = fit_and_score_sklearn_pipeline(sklearn_pipeline, train, test)
score_xorq = fitted_pipeline.score_expr(test)
assert score_xorq == score_sklearn  # Results should be identical
```

## Exploring Pipeline Lineage

One of Xorq's most powerful features is automatic lineage tracking. You can visualize the complete computational graph:

```{python}
from xorq.common.utils.lineage_utils import build_column_trees, print_tree

# Visualize lineage for predictions
print_tree(build_column_trees(expr)['predicted'])
```

This outputs a tree showing the complete lineage of how predictions were computed, including:

- Data source (Read operation from parquet file)
- Null value removal (DropNull)
- Feature selection (DropColumns)
- Train/test split logic (complex Filter with hash-based row sampling using row_number)
- Caching layer for performance optimization
- Final prediction computation (RemoteTable representing the fitted model output)

## Building and Running the Pipeline

### Build the Expression

Serialize your pipeline for deployment:

```bash
xorq build expr.py -e expr
```

Output:

```
Building expr from expr.py
Written 'expr' to builds/85f70392e095
builds/85f70392e095
```

The build creates several artifacts:
```
‚ùØ ls -a builds/85f70392e095/

58bf5c46c84f.sql  
aee2ca7e0e12.sql  
d5d0022edaf3.sql  
dacc6a158d67.sql  
deferred_reads.yaml  
expr.yaml  # Complete expression
metadata.json  
profiles.yaml  
sql.yaml # Debug SQL outputs
```

### Run the Built Expression

Execute the serialized Expression:

```bash
xorq run builds/85f70392e095/
```

Save results to different formats:
```bash
xorq run builds/85f70392e095/ --output-path predictions.csv --format csv
xorq run builds/85f70392e095/ --output-path predictions.parquet --format parquet
```

## Key Benefits Demonstrated

The penguins template showcases several xorq advantages:

**Deferred Execution**: ML pipelines are built as expressions and executed only when needed, enabling optimization and caching.
**Automatic Caching**: Train/test splits are cached automatically, avoiding recomputation across different experiments.
**Reproducibility**: Random seeds and versioned data ensure consistent results across runs.
**Lineage Tracking**: Complete computational provenance from raw data to final predictions.
**Portability**: Pipelines serialize to YAML/SQL artifacts that can be version controlled and deployed anywhere.
**Engine Flexibility**: Same code works across DuckDB, DataFusion, Snowflake, and other supported backends.

## Next Steps

- Experiment with the generated `expr.py` file
- Try different feature combinations by modifying the `features` tuple
- Explore different algorithms by changing the pipeline steps
- Read about [Profiles](https://docs.xorq.dev/core_concepts/profiles_guide) to connect to remote backends
- Learn about the [Caching system](https://docs.xorq.dev/core_concepts/caching) for automatic result storage
- Check out more examples in the [`examples/`](https://github.com/xorq-labs/xorq/tree/main/examples) directory
- Join our [community](https://discord.gg/8Kma9DhcJG) for support and discussions