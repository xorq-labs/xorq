---
title: 'Validate data in Xorq pipelines'
description: "Catch schema errors at expression creation and filter bad data at runtime"
---

This guide shows you how to validate data quality before it breaks your pipeline. You'll use schema validation to catch errors at expression creation and runtime validation to catch issues during execution.

## Prerequisites

- Xorq installed (see [Install Xorq](../../getting_started/installation.qmd))
- Completed [Quickstart](../../getting_started/quickstart.qmd)

## Validate schemas at expression creation

Data validation happens at two stages: when you create expressions and when you execute them. Start with schema validation, which catches type and column mismatches immediately when you build your pipeline, before any data processing begins.

Define required columns and types using `xo.schema()`. Create `schema_validation.py`:

```python
# schema_validation.py
import xorq.api as xo
import pandas as pd

# Define expected input schema
input_schema = xo.schema({
    "customer_id": "int64",
    "email": "string",
    "status": "string"
})

# Define processing function
def validate_schema(df: pd.DataFrame) -> pd.DataFrame:
    """Add validation flag to each row."""
    df = df.copy()
    df["validated"] = True
    return df

# Create data with the expected schema
con = xo.connect()
data = xo.memtable({
    "customer_id": [1, 2, 3],
    "email": ["a@b.com", "c@d.com", "e@f.com"],
    "status": ["active", "inactive", "pending"]
}, schema=input_schema, name="customers")

# Use the schema from the data expression for exact matching
actual_input_schema = data.schema()
actual_output_schema = xo.schema(actual_input_schema | {"validated": "bool"})

# Create UDXF with schema validation
# Schema validation happens here when creating the expression
# Raises ValueError immediately if schema doesn't match
validated = xo.expr.relations.flight_udxf(
    data,
    process_df=validate_schema,
    maybe_schema_in=actual_input_schema,
    maybe_schema_out=actual_output_schema,
    con=con,
    make_udxf_kwargs={"name": "SchemaValidator"}
)

df = validated.execute()
print(f"Validated schema for {len(df)} rows")
print(df)
```

Run the script:

```bash
python schema_validation.py
```

You should see a result like this:

```
Validated schema for 3 rows
   customer_id    email    status  validated
0            1  a@b.com    active       True
1            2  c@d.com  inactive       True
2            3  e@f.com   pending       True
```

If you create data with a missing column, then you'll see a schema validation error:

```python
# Missing customer_id column - this will fail
data = xo.memtable({
    "email": ["a@b.com"],
    "status": ["active"]
}, name="customers")

validated = xo.expr.relations.flight_udxf(
    data,
    process_df=validate_schema,
    maybe_schema_in=input_schema,  # Expects customer_id
    maybe_schema_out=actual_output_schema,
    con=con,
    make_udxf_kwargs={"name": "SchemaValidator"}
)
# Raises: ValueError: Schema validation failed, expected: ibis.Schema {
#   customer_id  int64
#   email        string
#   status       string
# } found: ibis.Schema {
#   email   string
#   status  string
# }
```

## Validate data at runtime

Runtime validation catches data quality issues during execution. These checks run when you call `.execute()` and fail before downstream processing continues.

### Check for null values

Remove rows with missing values in critical columns. Create `check_nulls.py`:

```python
# check_nulls.py
import xorq.api as xo

con = xo.connect()

data = xo.memtable({
    "customer_id": [1, 2, None, 4],
    "email": ["a@b.com", None, "c@d.com", "e@f.com"],
    "status": ["active", "inactive", None, "pending"]
}, name="customers")

# Filter out rows where critical columns are null
valid_data = data.filter(
    xo._.customer_id.notnull(),
    xo._.email.notnull(),
    xo._.status.notnull()
)

original = data.execute()
df = valid_data.execute()
print(f"Original rows: {len(original)}")
print(f"Valid rows (no nulls): {len(df)}")
print(f"Removed rows: {len(original) - len(df)}")
```

Run the script:

```bash
python check_nulls.py
```

You should see a result like this:

```
Original rows: 4
Valid rows (no nulls): 2
Removed rows: 2
```

### Stop processing on validation failure

Raise exceptions when validation fails to prevent processing invalid data. Create `fail_fast_validation.py`:

```python
# fail_fast_validation.py
import xorq.api as xo

def validate_and_process(data_expr, required_columns):
    """Validate data and fail fast if invalid."""
    # Check that required columns exist in schema
    schema = data_expr.schema()
    missing = [col for col in required_columns if col not in schema.names]
    if missing:
        raise ValueError(f"Missing required columns: {missing}")
    
    # Apply validation filters
    validated = data_expr.filter(
        *[data_expr[col].notnull() for col in required_columns]
    )
    
    # Execute and check if any rows remain
    result = validated.execute()
    if len(result) == 0:
        raise ValueError("No valid rows after validation")
    
    return result

con = xo.connect()
data = xo.memtable({
    "customer_id": [1, 2, None, 4],
    "email": ["a@b.com", None, "c@d.com", "e@f.com"],
    "status": ["active", "inactive", None, "pending"]
}, name="customers")

df = validate_and_process(data, ["customer_id", "email", "status"])
print(f"Processed {len(df)} valid rows")
```

Run the script:

```bash
python fail_fast_validation.py
```

You should see a result like this:

```
Processed 2 valid rows
```

If validation fails, then you'll see specific error messages:

**Missing columns:**
```python
data = xo.memtable({
    "customer_id": [1, 2],
    "email": ["a@b.com", "c@d.com"]
    # missing "status"
}, name="customers")

df = validate_and_process(data, ["customer_id", "email", "status"])
# Raises: ValueError: Missing required columns: ['status']
```

**No valid rows after filtering:**
```python
data = xo.memtable({
    "customer_id": [None, None],
    "email": [None, None],
    "status": [None, None]
}, name="customers")

df = validate_and_process(data, ["customer_id", "email", "status"])
# Raises: ValueError: No valid rows after validation
```

### Remove null rows

Drop rows with null values using `.drop_null()`. Create `drop_nulls.py`:

```python
# drop_nulls.py
import xorq.api as xo

con = xo.connect()

data = xo.memtable({
    "customer_id": [1, 2, None, 4],
    "email": ["a@b.com", None, "c@d.com", "e@f.com"]
}, name="customers")

# Remove rows where any column is null
clean_data = data.drop_null(how="any")

df = clean_data.execute()
print(f"Clean rows: {len(df)}")
```

Run the script:

```bash
python drop_nulls.py
```

You should see a result like this:

```
Clean rows: 2
```

The `how` parameter controls which rows to remove:

- `how="any"` removes rows with any null in any column
- `how="all"` removes only rows where every column is null
- `subset=["col1", "col2"]` removes rows where only specified columns are null

## Validate data ranges

Filter data to ensure values fall within expected ranges. Create `validate_ranges.py`:

```python
# validate_ranges.py
import xorq.api as xo

con = xo.connect()

data = xo.memtable({
    "age": [25, 150, 18, 10, 120],
    "revenue": [100.0, -10.0, 50.0, 200.0, 0.0]
}, name="customers")

# Ensure values are within expected bounds
valid_data = data.filter(
    (xo._.age >= 18) & (xo._.age <= 120),
    xo._.revenue > 0
)

df = valid_data.execute()
print(f"Rows within valid range: {len(df)}")
```

Run the script:

```bash
python validate_ranges.py
```

You should see a result like this:

```
Rows within valid range: 2
```

Range validation removes rows that don't meet the criteria. To verify, check that the output contains only values within the specified ranges and test edge cases such as age=18 or age=120.

## Validate string patterns

Check string formats using string methods. Create `validate_strings.py`:

```python
# validate_strings.py
import xorq.api as xo

con = xo.connect()

data = xo.memtable({
    "email": ["valid@test.com", "invalid", "missing.dot@com", "bad@", "good@test.com"]
}, name="customers")

# Check for valid email pattern
valid_emails = data.filter(
    xo._.email.contains("@"),
    xo._.email.contains(".")
)

df = valid_emails.execute()
print(f"Rows with valid email format: {len(df)}")
```

Run the script:

```bash
python validate_strings.py
```

You should see a result like this:

```
Rows with valid email format: 3
```

The script filters out emails that lack an "@" or a "." character. For production, use more comprehensive email validation rules.

## Create custom validation functions

Combine expression creation schema validation with runtime business rule validation using UDXFs. The schema check catches errors when creating the expression, and the function catches data quality issues during execution.

Create `custom_validation.py`:

```python
# custom_validation.py
import pandas as pd
import xorq.api as xo

# Define expected schema (compile-time validation)
input_schema = xo.schema({
    "customer_id": "int64",
    "email": "string",
    "status": "string"
})

# Define validation function (runtime validation)
def validate_customer_data(df: pd.DataFrame) -> pd.DataFrame:
    """Validate customer data and filter invalid rows."""
    # Validate business rules
    valid_statuses = ["active", "inactive", "pending"]
    valid_df = df[df["status"].isin(valid_statuses)]
    
    # Raise error if no valid rows remain
    if len(valid_df) == 0:
        raise ValueError("No valid rows after validation")
    
    return valid_df

# Apply validation
con = xo.connect()
data = xo.memtable({
    "customer_id": [1, 2, 3, 4],
    "email": ["a@b.com", "c@d.com", "e@f.com", "g@h.com"],
    "status": ["active", "inactive", "invalid", "pending"]
}, schema=input_schema, name="customers")

# Use the schema from the data expression for exact matching
actual_schema = data.schema()

# Create UDXF with schema validation
# Schema validation happens here when creating the expression
validated_data = xo.expr.relations.flight_udxf(
    data,
    process_df=validate_customer_data,
    maybe_schema_in=actual_schema,
    maybe_schema_out=actual_schema,  # Output has same schema (just filtered rows)
    con=con,
    make_udxf_kwargs={"name": "CustomerValidator"}
)

# Business rule validation happens at execute (during execution)
df = validated_data.execute()
print(f"Validated {len(df)} rows with custom rules")
print(df)
```

Run the script:

```bash
python custom_validation.py
```

You should see a result like this:

```
Validated 3 rows with custom rules
   customer_id    email    status
0            1  a@b.com    active
1            2  c@d.com  inactive
2            4  g@h.com   pending
```

The validation catches invalid data at different stages:

**Schema mismatch (at expression creation):**
```python
# If data is missing required columns
# Raises: ValueError: Schema validation failed, expected: ibis.Schema {...}
```

**Invalid status values (at execution):**
```python
# Data with status="invalid" is filtered out silently
# Only rows with valid statuses ("active", "inactive", "pending") are returned
```

**All rows invalid (at execution):**
```python
# If all rows have invalid status values
# Raises: ValueError: No valid rows after validation
```

## Production considerations

Choose the right validation strategy based on when you need to catch errors and how much data you're processing.

### Choose validation timing

**Expression creation validation (schema checks)**: Use for critical schema mismatches that would break your pipeline. These catch errors when you create the UDXF expression, before any data processing starts.

**Runtime validation (filters and assertions)**: Use for null checks, range validation, and business rules that filter rows. These catch data quality issues when you call `.execute()`.

For large datasets: Use schema validation in UDXFs for expression creation checks and filter-based validation for runtime data quality checks.

### Performance

Choosing the right validation timing affects performance. Understanding the overhead of each approach helps you optimize your pipeline.

**Expression creation validation** adds minimal overhead because it checks types once when you create the UDXF, not during execution.

**Runtime validation** processes every row during execution, so filter early in your pipeline to reduce data volume for downstream operations.

### Monitoring

Performance optimization helps, but monitoring tells you when validation fails in production. Track validation metrics to identify data quality issues before they break your pipeline.

| Metric | What to monitor |
|--------|-----------------|
| Validation failure rate | Percentage of rows filtered out |
| Schema mismatch frequency | How often schema validation fails |
| Null value percentage | Percentage of nulls in critical columns |

Log validation failures to identify data quality trends and upstream issues.

## Next steps

You now have expression creation schema validation to catch errors before execution and runtime validation to filter bad data during execution. For troubleshooting validation errors, see [Troubleshoot common errors](../../troubleshooting/common_errors.qmd).

- [Transform data with custom UDFs](transform_data_with_custom_udfs.qmd) - Create reusable transformations
- [Build feature pipelines](build_feature_pipelines.qmd) - Create production feature pipelines
- [Optimize pipeline performance](../../guides/performance_workflows/optimize_pipeline_performance.qmd) - Improve validation performance