---
title: 'Read and write production data'
description: "Handle data I/O reliably in production with error handling, monitoring, and cost optimization"
---

This guide shows you how to read from and write to production data sources with Xorq.

## Prerequisites

- Xorq installed (see [Install Xorq](../../getting_started/install_xorq.qmd))
- Completed [Quickstart](../../getting_started/quickstart.qmd)
- Access to your production data source ([AWS S3](https://docs.aws.amazon.com/s3/), [Google Cloud Storage](https://cloud.google.com/storage/docs), or [PostgreSQL](https://www.postgresql.org/docs/))

### Install cloud storage dependencies

If you need S3 or GCS access, then install the required packages:

::: {.panel-tabset}

### S3 access

```bash
pip install s3fs
```

### GCS access

```bash
pip install gcsfs
```

### Both

```bash
pip install s3fs gcsfs
```

:::

## Read from cloud storage

If you need to read CSV or Parquet files from S3 or GCS, then connect to a backend and use deferred reading:

```python
import xorq.api as xo

# <1>
con = xo.connect()  # For production with credentials
# For public buckets: con = xo.duckdb.connect()

# <2>
data = xo.deferred_read_csv(
    path="s3://my-bucket/data/customers.csv",
    con=con,
    table_name="customers",
    storage_options={"aws.region": "us-east-1"}  # Required for default backend
)

# <3>
df = data.execute()  # Execute when ready
```
1. Connect to the backend. Use `xo.duckdb.connect()` for public buckets.
2. Read CSV from S3. Add `storage_options` with AWS region for default backend.
3. Execute the deferred operation to get the data.

If you need to read Parquet files, then use `xo.deferred_read_parquet()` instead:

```python
data = xo.deferred_read_parquet(
    path="gs://my-bucket/data/transactions.parquet",
    con=con,
    table_name="transactions"
)
df = data.execute()
```

If you're reading public buckets without credentials, then use the DuckDB backend:

```python
con = xo.duckdb.connect()  # DuckDB can access public buckets
data = xo.deferred_read_csv(
    path="s3://my-bucket/data/customers.csv",
    con=con,
    table_name="customers"
)
```

If you encounter CSV parsing errors, then use `strict_mode=False` or `ignore_errors=True`:

```python
con = xo.duckdb.connect()
data = xo.deferred_read_csv(
    path="s3://my-bucket/data.csv",
    con=con,
    table_name="data",
    strict_mode=False  # Allow reading malformed CSV rows
    # Or: ignore_errors=True  # Skip rows with parsing errors
)
```

## Read from databases

If you need to read from PostgreSQL, then connect using environment variables:

```python
import xorq.api as xo

# <1>
pg_con = xo.postgres.connect_env()  # Uses environment variables

# <2>
data = pg_con.table("customers")
# Or: data = pg_con.sql("SELECT * FROM customers WHERE created_at > '2024-01-01'")

# <3>
df = data.execute()
```
1. Connect to PostgreSQL using environment variables for credentials.
2. Read from a table or execute a SQL query. Returns a deferred expression.
3. Execute the deferred operation to get the actual data.

If you need to store database credentials, then use environment variables rather than hardcoding them in your scripts.

## Write to cloud storage

If you need to write results to S3 or GCS, then connect to a backend and write in Parquet format:

```python
import xorq.api as xo

# <1>
con = xo.connect()

# <2>
results = data.filter(xo._.status == "active").agg(total=xo._.amount.sum())

# <3>
results.to_parquet("s3://my-bucket/output/results.parquet")
```
1. Connect to the backend. Required for cloud storage operations.
2. Process the data. This is still deferred.
3. Write results to S3 in Parquet format. The write happens immediately.

If you need to cache intermediate results for faster subsequent reads, then use the caching API:

```python
import xorq.api as xo
from xorq.caching import ParquetCache

con = xo.connect()
cache = ParquetCache.from_kwargs(source=con)

cached_results = results.cache(cache=cache)
cached_results.to_parquet("s3://my-bucket/output/results.parquet")
```

If you need to avoid overwriting production data, then use versioned paths for outputs.

## Write to databases

If you need to write results to PostgreSQL, then connect and create a table:

```python
import xorq.api as xo

pg_con = xo.postgres.connect_env()

# Create or overwrite a table
pg_con.create_table("results_table", data, overwrite=True)  # or False to append

# Register a temporary table for intermediate results
pg_con.register("temp_results", data)
```

:::{.callout-warning}
If you set `overwrite=True`, then `create_table` permanently deletes any existing table with that name. Use `overwrite=False` to append data, but make sure your schema matches the existing table.
:::

## Handle connection failures

If you encounter transient failures like network timeouts, then wrap I/O operations in retry logic:

```python
import time
import xorq.api as xo
from pyarrow import ArrowIOError

def read_with_retry(path, con=None, max_attempts=3, initial_backoff=2):
    """Read data with exponential backoff retry."""
    if con is None:
        con = xo.connect()
    
    for attempt in range(max_attempts):
        try:
            data = xo.deferred_read_csv(path=path, con=con)
            return data.execute()
        except ArrowIOError as e:
            if attempt < max_attempts - 1:
                wait_time = initial_backoff ** (attempt + 1)
                print(f"Connection failed (attempt {attempt + 1}/{max_attempts}), retrying in {wait_time}s...")
                time.sleep(wait_time)
            else:
                raise RuntimeError(f"Failed after {max_attempts} attempts: {e}")

con = xo.connect()
df = read_with_retry("s3://my-bucket/data.csv", con=con)
```

:::{.callout-note}
Retry transient errors like timeouts and network issues, but don't retry permanent errors like authentication failures or file-not-found errors.
:::

## Configure authentication

If you need to read from cloud storage, then set up credentials in your environment:

**For AWS S3:**
```bash
export AWS_ACCESS_KEY_ID=your_access_key
export AWS_SECRET_ACCESS_KEY=your_secret_key
export AWS_DEFAULT_REGION=us-east-1
```

**For Google Cloud Storage:**
```bash
export GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account-key.json
```

**For EC2/ECS (recommended):** Attach an IAM role to your EC2 instance or ECS task. Credentials are automatically retrieved from instance metadata.

:::{.callout-warning}
Never commit credentials to code or version control. Use IAM roles instead of access keys when possible, rotate credentials regularly, and configure least-privilege permissions.
:::

## Handle large files

If your files are larger than 1GB, then use batch processing to avoid exhausting available memory:

```python
import xorq.api as xo

con = xo.connect()
data = xo.deferred_read_parquet(
    path="s3://my-bucket/large-file.parquet",
    con=con
)

# Process data in chunks
for batch in data.to_pyarrow_batches(chunk_size=100000):
    df = batch.to_pandas()
    process_batch(df)
```

:::{.callout-tip}
For files larger than 1GB, always use streaming. Loading entire large files into memory will crash your process with out-of-memory errors.
:::

If you need to optimize storage costs, then use Parquet format instead of CSV. Parquet files compress 50-80% more than CSV and preserve data types.

## Monitor performance

If you need to track I/O performance, then wrap operations with timing and logging:

```python
import time
import logging
import xorq.api as xo

logger = logging.getLogger(__name__)

def monitored_read(path):
    """Read data with performance tracking."""
    start_time = time.time()
    
    try:
        data = xo.deferred_read_csv(path=path, con=xo.connect())
        df = data.execute()
        
        read_time = time.time() - start_time
        row_count = len(df)
        size_mb = df.memory_usage(deep=True).sum() / 1024 / 1024
        
        logger.info(f"Read {row_count:,} rows ({size_mb:.2f} MB) in {read_time:.2f}s")
        return df
    except Exception as e:
        read_time = time.time() - start_time
        logger.error(f"Read failed after {read_time:.2f}s: {e}")
        raise

df = monitored_read("s3://my-bucket/data.csv")
```

## Troubleshoot common errors

If you see a connection timeout, then use streaming for large files, check network connectivity, or switch to a faster backend (DuckDB handles large files well).

If you see a file not found error, then verify the path with `aws s3 ls s3://bucket/path/` or `gsutil ls gs://bucket/path/`. Check for typos and verify permissions.

If you see an out of memory error, then switch to streaming with `data.to_pyarrow_batches(chunk_size=100000)`. Reduce chunk size if already streaming. Process one file at a time.

If you see a schema mismatch error, then inspect the schema with `data.schema()` or read the first few rows. Handle missing columns gracefully:

```python
# For pandas DataFrames
available_cols = [c for c in expected_cols if c in data.columns]
data = data[available_cols]

# For Xorq expressions
available_cols = [c for c in expected_cols if c in data.columns]
data = data.select(available_cols)
```

If you see an authentication failed error, then verify credentials with `print(os.environ.get("AWS_ACCESS_KEY_ID"))`. Check IAM permissions grant access to your bucket. Use IAM roles instead of access keys when possible.

## Optimize costs

If you need to reduce storage costs, then use Parquet format instead of CSV. For 1TB, CSV costs ~$23/month while Parquet costs ~$5-10/month.

If you need to reduce data transfer costs, then cache results using `data.cache(cache=ParquetCache.from_kwargs(source=con))`. This saves 99% on transfer costs for repeated reads.

If you need to choose the right backend for your data size, then use Pandas for data under 1GB, DuckDB for 1-100GB, and PostgreSQL for data over 100GB or concurrent access.

## Next steps

- [Build data quality checks](build_data_quality_checks.qmd) - Validate data before processing
- [Transform data with custom UDFs](transform_data_with_custom_udfs.qmd) - Create reusable transformations
- [Optimize pipeline performance](../../guides/performance_workflows/optimize_pipeline_performance.qmd) - Improve I/O performance with caching
- [Handle production errors](../../guides/platform_workflows/handle_production_errors.qmd) - Implement comprehensive error handling
