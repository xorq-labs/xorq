---
title: 'Read and write production data'
description: "Handle data I/O reliably in production with error handling, monitoring, and cost optimization"
---

This guide shows you how to read from and write to production data sources with Xorq using S3, GCS, and PostgreSQL. You'll set up authentication, read and transform data, write results, handle errors, and monitor performance.

## Prerequisites

- Xorq installed (see [Install Xorq](../../getting_started/install_xorq.qmd))
- Completed [Quickstart](../../getting_started/quickstart.qmd)
- Access to [AWS S3](https://docs.aws.amazon.com/s3/), [Google Cloud Storage](https://cloud.google.com/storage/docs), or [PostgreSQL](https://www.postgresql.org/docs/)
- Cloud storage dependencies: `pip install s3fs` (for S3) or `pip install gcsfs` (for GCS)

## Configure authentication

Set credentials as environment variables so Xorq can securely access your data sources. The connection reads these automatically when you connect to backends.

::: {.panel-tabset}

### AWS S3

```bash
export AWS_ACCESS_KEY_ID=your_access_key
export AWS_SECRET_ACCESS_KEY=your_secret_key
export AWS_DEFAULT_REGION=us-east-1
```

### Google Cloud Storage

```bash
export GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account-key.json
```

### PostgreSQL

```bash
export POSTGRES_HOST=your-db-host.com
export POSTGRES_PORT=5432
export POSTGRES_USER=your_username
export POSTGRES_PASSWORD=your_password
export POSTGRES_DB=your_database
```

:::

For production deployments: Use IAM roles on EC2/ECS instead of access keys. The instance retrieves credentials automatically from metadata.

If you encounter authentication errors, then see [Troubleshoot authentication](../../troubleshooting/authentication-errors.qmd).

## Read from cloud storage

Connect to a backend and use deferred reading to load data from S3 or GCS. The operation doesn't execute until you call `execute()`.

:::{.callout-note}
Before running these examples, ensure you have:

- An S3 or GCS bucket created
- Test data files uploaded to your bucket (example: `customers.csv`)
- Replace `my-bucket` in the examples with your actual bucket name
- Network access to AWS/GCS services configured
:::

Create a file called `read_s3.py`:

```python
# read_s3.py
import xorq.api as xo

con = xo.connect()

customers = xo.deferred_read_csv(
    path="s3://my-bucket/data/customers.csv",
    con=con,
    table_name="customers"
)

df = customers.execute()
print(f"Loaded {len(df)} customers")
```

Run the script:

```bash
python read_s3.py
```

You should see a result like this:

```
Loaded 10000 customers
```

### Variations

Use these variations for different file formats and storage providers:

#### Parquet files

Parquet format is recommended for production workloads.

```python
customers = xo.deferred_read_parquet(
    path="s3://my-bucket/data/customers.parquet",
    con=con,
    table_name="customers"
)
```

#### Google Cloud Storage

```python
customers = xo.deferred_read_parquet(
    path="gs://my-bucket/data/customers.parquet",
    con=con,
    table_name="customers"
)
```

#### Public buckets without credentials

```python
con = xo.duckdb.connect()  # DuckDB can access public buckets
customers = xo.deferred_read_csv(
    path="s3://my-bucket/data/customers.csv",
    con=con,
    table_name="customers"
)
```

If you see connection timeouts or file-not-found errors, then see [Troubleshoot data I/O errors](../../troubleshooting/data-io-errors.qmd).

## Transform data

Apply filters and aggregations to your data before executing. All transformations remain deferred until you call `execute()`.

Create `transform_data.py`:

```python
# transform_data.py
import xorq.api as xo

con = xo.connect()

customers = xo.deferred_read_csv(
    path="s3://my-bucket/data/customers.csv",
    con=con,
    table_name="customers"
)

active_customers = customers.filter(xo._.status == "active")

summary = active_customers.agg(
    total_customers=xo._.customer_id.count(),
    total_revenue=xo._.revenue.sum()
)

df = summary.execute()
print(df)
```

The pipeline executes when you call `execute()`: read → filter → aggregate → return results.

## Write to cloud storage

Write processed data back to S3 or GCS in Parquet format for optimal storage and performance.

Create `write_s3.py`:

```python
# write_s3.py
import xorq.api as xo

con = xo.connect()

customers = xo.deferred_read_csv(
    path="s3://my-bucket/data/customers.csv",
    con=con,
    table_name="customers"
)

active_summary = customers.filter(
    xo._.status == "active"
).agg(
    total_customers=xo._.customer_id.count(),
    total_revenue=xo._.revenue.sum()
)

active_summary.to_parquet("s3://my-bucket/output/customer-summary.parquet")
```

To avoid overwriting existing data, use timestamped paths:

```python
from datetime import datetime

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
output_path = f"s3://my-bucket/output/summary-{timestamp}.parquet"
active_summary.to_parquet(output_path)
```

## Read from PostgreSQL

Beyond cloud storage, Xorq also works directly with databases. Connect to PostgreSQL to query tables and execute SQL, using the same deferred execution model you've seen with cloud files.

:::{.callout-note}
Before running PostgreSQL examples, ensure you have:

- PostgreSQL server running and accessible
- Database created with test tables (example: `customers`, `orders`)
- Environment variables configured (`POSTGRES_HOST`, `POSTGRES_PORT`, `POSTGRES_USER`, `POSTGRES_PASSWORD`, `POSTGRES_DB`)
:::

Create `read_postgres.py`:

```python
# read_postgres.py
import xorq.api as xo

pg_con = xo.postgres.connect_env()

customers = pg_con.table("customers")
active_customers = customers.filter(xo._.status == "active")

df = active_customers.execute()
print(f"Found {len(df)} active customers")
```

For raw SQL queries:

```python
recent_orders = pg_con.sql("""
    SELECT customer_id, order_date, total
    FROM orders
    WHERE order_date > '2024-01-01'
""")
df = recent_orders.execute()
```

## Write to PostgreSQL

Create PostgreSQL tables from Xorq expressions or register temporary tables for intermediate results.

Create `write_postgres.py`:

```python
# write_postgres.py
import xorq.api as xo

pg_con = xo.postgres.connect_env()

customers = xo.deferred_read_csv(
    path="s3://my-bucket/data/customers.csv",
    con=xo.connect(),
    table_name="customers"
)

active_customers = customers.filter(xo._.status == "active")

pg_con.create_table("active_customers", active_customers, overwrite=True)
```

Set `overwrite=True` to replace existing tables. Set `overwrite=False` (default) to raise an error if the table already exists.

For temporary tables that exist only for the current session:

```python
pg_con.register("temp_table", data)
```

## Handle errors

Now that you've seen the core read/write patterns, production deployments need reliability. Network failures, timeouts, and transient errors are common in cloud I/O, so add retry logic to handle them gracefully.

Create `retry_read.py`:

```python
# retry_read.py
import time
import xorq.api as xo
from pyarrow import ArrowIOError

def read_with_retry(path, con=None, max_attempts=3, initial_backoff=2):
    if con is None:
        con = xo.connect()
    
    for attempt in range(max_attempts):
        try:
            data = xo.deferred_read_csv(path=path, con=con)
            return data.execute()
        except ArrowIOError as e:
            if attempt < max_attempts - 1:
                wait_time = initial_backoff ** (attempt + 1)
                print(f"Retry {attempt + 1}/{max_attempts} in {wait_time}s...")
                time.sleep(wait_time)
            else:
                raise RuntimeError(f"Failed after {max_attempts} attempts: {e}")

con = xo.connect()
df = read_with_retry("s3://my-bucket/data/customers.csv", con=con)
```

Retry network timeouts and connection resets. Don't retry authentication failures or file-not-found errors.

## Handle large files

Stream data in chunks for files larger than 1GB to keep memory usage constant regardless of file size.

Create `stream_large.py`:

```python
# stream_large.py
import xorq.api as xo

con = xo.connect()

data = xo.deferred_read_parquet(
    path="s3://my-bucket/large-file.parquet",
    con=con
)

total_rows = 0
for batch in data.to_pyarrow_batches(chunk_size=100000):
    df = batch.to_pandas()
    total_rows += len(df)
    print(f"Processed batch: {len(df)} rows")

print(f"Total: {total_rows} rows")
```

Reduce `chunk_size` to 10,000 if you run out of memory. For persistent memory issues, see [Troubleshoot memory errors](../../troubleshooting/memory-errors.qmd).

## Monitor performance

Track I/O metrics to identify bottlenecks and optimize pipeline performance.

Create `monitor_io.py`:

```python
# monitor_io.py
import time
import logging
import xorq.api as xo

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

def monitored_read(path, con=None):
    if con is None:
        con = xo.connect()
    
    start = time.time()
    
    try:
        data = xo.deferred_read_csv(path=path, con=con)
        df = data.execute()
        
        elapsed = time.time() - start
        rows = len(df)
        mb = df.memory_usage(deep=True).sum() / 1024 / 1024
        
        logger.info(f"Read {rows:,} rows ({mb:.2f} MB) in {elapsed:.2f}s")
        logger.info(f"Throughput: {rows / elapsed:,.0f} rows/sec")
        return df
    except Exception as e:
        logger.error(f"Read failed: {e}")
        raise

df = monitored_read("s3://my-bucket/data/customers.csv")
```

## Production considerations

Consider these factors when deploying to production:

### Performance

Choose backends based on data size:

| Data Size | Backend | Rationale |
|-----------|---------|-----------|
| <1GB | Pandas | Simple, fast for small data |
| 1-100GB | DuckDB | Optimized for analytics, efficient file handling |
| >100GB | PostgreSQL | Handles large datasets, concurrent operations |

Use Parquet for production. It's 50-80% smaller than CSV and reads 3-5x faster.

### Security

Performance optimization helps you scale, but security prevents data breaches. Configure authentication and access controls properly before deploying to production.

Use IAM roles on EC2/ECS instead of access keys. Rotate credentials every 90 days if using access keys and configure least-privilege permissions. Never commit credentials to code.

### Monitoring

Security protects your data, but monitoring tells you when systems fail. Track I/O metrics to identify bottlenecks before they impact users.

| Metric | What to Track |
|--------|---------------|
| Read latency | Time to load and execute queries |
| Write latency | Time to write results to storage |
| Error rate | Percentage of failed operations |
| Memory usage | Peak memory during execution |

Set alert thresholds based on your application's service-level agreements (SLAs) and baseline performance. Log authentication failures, file-not-found errors, slow operations, and retry attempts.

### Cost optimization

Monitoring shows you what's happening, but cost optimization ensures you're not overpaying for it. Storage and compute costs add up quickly at scale.

- CSV on S3 Standard: ~$23/month
- Parquet on S3 Standard: ~$5-10/month

Use Parquet for 50-80% storage savings. Cache frequently accessed data locally to reduce transfer costs. Keep compute and storage in the same region.

### Scaling

Cost optimization reduces waste, but scaling ensures systems handle growing data volumes, such as planning capacity based on your data size and growth rate.

**Backend selection for scale**:

- **<1GB**: Pandas backend works well for small datasets
- **1-100GB**: DuckDB provides efficient analytics with lower memory overhead
- **>100GB**: PostgreSQL handles large datasets with concurrent access

**Performance varies by**:

- Hardware (CPU, memory, disk I/O)
- Network latency between compute and storage
- File format (Parquet is 3-5x faster than CSV)
- Query complexity and data transformations

**Capacity planning**: Start with instance memory at least 2x your dataset size. Monitor actual usage and adjust based on your specific workload.

## Troubleshooting

Even with proper authentication, error handling, and monitoring, you'll encounter issues when working with production data. Here are the most common problems and their solutions.

### Authentication fails

**Problem**: `botocore.exceptions.NoCredentialsError: Unable to locate credentials`

**Cause**: AWS credentials not configured or expired.

**Solution**: Verify environment variables are set:

```bash
echo $AWS_ACCESS_KEY_ID
echo $AWS_SECRET_ACCESS_KEY
```

If on EC2, use IAM roles instead of access keys:

```bash
# Remove access keys from environment
unset AWS_ACCESS_KEY_ID
unset AWS_SECRET_ACCESS_KEY

# Attach IAM role to EC2 instance via AWS Console
# Xorq will use instance metadata to get credentials
```

### File not found

**Problem**: `FileNotFoundError: [Errno 2] No such file or directory`

**Cause**: Incorrect file path or missing file.

**Solution**: Verify the file exists, and the path is correct:

```bash
# List bucket contents
aws s3 ls s3://my-bucket/data/

# Test file access
aws s3 cp s3://my-bucket/data/customers.csv - | head
```

Use absolute paths with bucket prefix:

```python
# Correct: Full path with bucket
path = "s3://my-bucket/data/customers.csv"

# Incorrect: Relative path
path = "data/customers.csv"
```

### Out of memory

**Problem**: `MemoryError` when reading large files.

**Cause**: Loading the entire file into memory at once.

**Solution**: Use streaming for files >1GB:

```python
# Stream data in chunks
for batch in data.to_pyarrow_batches(chunk_size=10000):
    process_batch(batch)  # Process each chunk separately
```

If still failing, reduce chunk size to 1,000 or increase instance memory.

### Slow read performance

**Problem**: Reads take longer than expected for your dataset size.

**Cause**: Network latency, wrong backend, or inefficient file format.

**Solution**: Profile and optimize:

```python
import time

start = time.time()
df = data.execute()
read_time = time.time() - start

print(f"Read time: {read_time:.2f}s for {len(df)} rows")
```

Optimize based on results:

- **Slow file reads**: Switch from CSV to Parquet (3-5x faster)
- **High network latency**: Move compute and storage to the same region
- **Large datasets**: Use DuckDB backend for better memory efficiency
- **Files >1GB**: Use streaming with `to_pyarrow_batches()`

## Next steps

You now have working pipelines that read from production sources, transform data with Xorq, write results with error handling, and track performance metrics.

- [Build data quality checks](build_data_quality_checks.qmd) - Validate data before processing
- [Transform data with custom UDFs](transform_data_with_custom_udfs.qmd) - Create reusable transformations
- [Optimize pipeline performance](../../guides/performance_workflows/optimize_pipeline_performance.qmd) - Improve I/O performance
- [Handle production errors](../../guides/platform_workflows/handle_production_errors.qmd) - Implement error handling
