---
title: 'Read and write production data'
description: "Handle data I/O reliably in production with error handling, monitoring, and cost optimization"
---

This guide shows you how to read from and write to production data sources with Xorq using S3, GCS, and PostgreSQL. You'll set up authentication, read and transform data, write results, handle errors, and monitor performance.

## Prerequisites

- Xorq installed (see [Install Xorq](../../getting_started/install_xorq.qmd))
- Completed [Quickstart](../../getting_started/quickstart.qmd)
- Access to [AWS S3](https://docs.aws.amazon.com/s3/), [Google Cloud Storage](https://cloud.google.com/storage/docs), or [PostgreSQL](https://www.postgresql.org/docs/)
- Cloud storage dependencies: `pip install s3fs` (for S3) or `pip install gcsfs` (for GCS)

## Configure authentication

Set credentials as environment variables so Xorq can access your data sources securely. The connection reads these automatically when you connect to backends.

::: {.panel-tabset}

### AWS S3

```bash
export AWS_ACCESS_KEY_ID=your_access_key
export AWS_SECRET_ACCESS_KEY=your_secret_key
export AWS_DEFAULT_REGION=us-east-1
```

### Google Cloud Storage

```bash
export GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account-key.json
```

### PostgreSQL

```bash
export POSTGRES_HOST=your-db-host.com
export POSTGRES_PORT=5432
export POSTGRES_USER=your_username
export POSTGRES_PASSWORD=your_password
export POSTGRES_DB=your_database
```

:::

For production deployments: Use IAM roles on EC2/ECS instead of access keys. The instance retrieves credentials automatically from metadata.

If you encounter authentication errors, see [Troubleshoot authentication](../../troubleshooting/authentication-errors.qmd).

## Read from cloud storage

Connect to a backend and use deferred reading to load data from S3 or GCS. The operation doesn't execute until you call `execute()`.

Create a file called `read_s3.py`:

```python
# read_s3.py
import xorq.api as xo

con = xo.connect()

customers = xo.deferred_read_csv(
    path="s3://my-bucket/data/customers.csv",
    con=con,
    table_name="customers"
)

df = customers.execute()
print(f"Loaded {len(df)} customers")
```

Run the script:

```bash
python read_s3.py
```

You should see a result like this:

```
Loaded 10000 customers
```

### Variations

Use these variations for different file formats and storage providers:

#### Parquet files

Parquet format is recommended for production workloads.

```python
customers = xo.deferred_read_parquet(
    path="s3://my-bucket/data/customers.parquet",
    con=con,
    table_name="customers"
)
```

#### Google Cloud Storage

```python
customers = xo.deferred_read_parquet(
    path="gs://my-bucket/data/customers.parquet",
    con=con,
    table_name="customers"
)
```

#### Public buckets without credentials

```python
con = xo.duckdb.connect()  # DuckDB can access public buckets
customers = xo.deferred_read_csv(
    path="s3://my-bucket/data/customers.csv",
    con=con,
    table_name="customers"
)
```

If you see connection timeouts or file-not-found errors, see [Troubleshoot data I/O errors](../../troubleshooting/data-io-errors.qmd).

## Transform data

Apply filters and aggregations to your data before executing. All transformations remain deferred until you call `execute()`.

Create `transform_data.py`:

```python
# transform_data.py
import xorq.api as xo

con = xo.connect()

customers = xo.deferred_read_csv(
    path="s3://my-bucket/data/customers.csv",
    con=con,
    table_name="customers"
)

active_customers = customers.filter(xo._.status == "active")

summary = active_customers.agg(
    total_customers=xo._.customer_id.count(),
    total_revenue=xo._.revenue.sum()
)

df = summary.execute()
print(df)
```

The pipeline executes when you call `execute()`: read → filter → aggregate → return results.

## Write to cloud storage

Write processed data back to S3 or GCS in Parquet format for optimal storage and performance.

Create `write_s3.py`:

```python
# write_s3.py
import xorq.api as xo

con = xo.connect()

customers = xo.deferred_read_csv(
    path="s3://my-bucket/data/customers.csv",
    con=con,
    table_name="customers"
)

active_summary = customers.filter(
    xo._.status == "active"
).agg(
    total_customers=xo._.customer_id.count(),
    total_revenue=xo._.revenue.sum()
)

active_summary.to_parquet("s3://my-bucket/output/customer-summary.parquet")
```

To avoid overwriting existing data, use timestamped paths:

```python
from datetime import datetime

timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
output_path = f"s3://my-bucket/output/summary-{timestamp}.parquet"
active_summary.to_parquet(output_path)
```

## Read from PostgreSQL

Connect to PostgreSQL and reference tables or execute SQL queries. The connection uses environment variables you configured earlier.

Create `read_postgres.py`:

```python
# read_postgres.py
import xorq.api as xo

pg_con = xo.postgres.connect_env()

customers = pg_con.table("customers")
active_customers = customers.filter(xo._.status == "active")

df = active_customers.execute()
print(f"Found {len(df)} active customers")
```

For raw SQL queries:

```python
recent_orders = pg_con.sql("""
    SELECT customer_id, order_date, total
    FROM orders
    WHERE order_date > '2024-01-01'
""")
df = recent_orders.execute()
```

## Write to PostgreSQL

Create PostgreSQL tables from Xorq expressions or register temporary tables for intermediate results.

Create `write_postgres.py`:

```python
# write_postgres.py
import xorq.api as xo

pg_con = xo.postgres.connect_env()

customers = xo.deferred_read_csv(
    path="s3://my-bucket/data/customers.csv",
    con=xo.connect(),
    table_name="customers"
)

active_customers = customers.filter(xo._.status == "active")

pg_con.create_table("active_customers", active_customers, overwrite=True)
```

Set `overwrite=True` to replace existing tables or `overwrite=False` to append rows.

For temporary tables that exist only for the current session:

```python
pg_con.register("temp_table", data)
```

## Handle errors

Add retry logic with exponential backoff to handle transient network failures gracefully.

Create `retry_read.py`:

```python
# retry_read.py
import time
import xorq.api as xo
from pyarrow import ArrowIOError

def read_with_retry(path, con=None, max_attempts=3, initial_backoff=2):
    if con is None:
        con = xo.connect()
    
    for attempt in range(max_attempts):
        try:
            data = xo.deferred_read_csv(path=path, con=con)
            return data.execute()
        except ArrowIOError as e:
            if attempt < max_attempts - 1:
                wait_time = initial_backoff ** (attempt + 1)
                print(f"Retry {attempt + 1}/{max_attempts} in {wait_time}s...")
                time.sleep(wait_time)
            else:
                raise RuntimeError(f"Failed after {max_attempts} attempts: {e}")

con = xo.connect()
df = read_with_retry("s3://my-bucket/data/customers.csv", con=con)
```

Retry network timeouts and connection resets. Don't retry authentication failures or file-not-found errors.

## Handle large files

Stream data in chunks for files larger than 1GB to keep memory usage constant regardless of file size.

Create `stream_large.py`:

```python
# stream_large.py
import xorq.api as xo

con = xo.connect()

data = xo.deferred_read_parquet(
    path="s3://my-bucket/large-file.parquet",
    con=con
)

total_rows = 0
for batch in data.to_pyarrow_batches(chunk_size=100000):
    df = batch.to_pandas()
    total_rows += len(df)
    print(f"Processed batch: {len(df)} rows")

print(f"Total: {total_rows} rows")
```

Reduce `chunk_size` to 10,000 if you run out of memory. For persistent memory issues, see [Troubleshoot memory errors](../../troubleshooting/memory-errors.qmd).

## Monitor performance

Track I/O metrics to identify bottlenecks and optimize pipeline performance.

Create `monitor_io.py`:

```python
# monitor_io.py
import time
import logging
import xorq.api as xo

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

def monitored_read(path, con=None):
    if con is None:
        con = xo.connect()
    
    start = time.time()
    
    try:
        data = xo.deferred_read_csv(path=path, con=con)
        df = data.execute()
        
        elapsed = time.time() - start
        rows = len(df)
        mb = df.memory_usage(deep=True).sum() / 1024 / 1024
        
        logger.info(f"Read {rows:,} rows ({mb:.2f} MB) in {elapsed:.2f}s")
        logger.info(f"Throughput: {rows / elapsed:,.0f} rows/sec")
        return df
    except Exception as e:
        logger.error(f"Read failed: {e}")
        raise

df = monitored_read("s3://my-bucket/data/customers.csv")
```

## Production considerations

Consider these factors when deploying to production:

### Performance

Choose backends based on data size:

| Data Size | Backend | Rationale |
|-----------|---------|-----------|
| <1GB | Pandas | Simple, fast for small data |
| 1-100GB | DuckDB | Optimized for analytics, efficient file handling |
| >100GB | PostgreSQL | Handles large datasets, concurrent operations |

Use Parquet for production. It's 50-80% smaller than CSV and reads 3-5x faster.

### Security

Use IAM roles on EC2/ECS instead of access keys. Rotate credentials every 90 days if using access keys. Configure least-privilege permissions. Never commit credentials to code.

### Monitoring

Track these metrics in production:

| Metric | Alert Threshold |
|--------|-----------------|
| Read latency | >10s for <1GB files |
| Write latency | >30s for <1GB files |
| Error rate | >5% |
| Memory usage | >80% of available RAM |

Log authentication failures, file-not-found errors, operations exceeding latency thresholds, and retry attempts.

### Cost optimization

Storage costs for 1TB dataset:

- CSV on S3 Standard: ~$23/month
- Parquet on S3 Standard: ~$5-10/month

Use Parquet for 50-80% storage savings. Cache frequently accessed data locally to reduce transfer costs. Keep compute and storage in the same region.

### Scaling

- At 10GB scale: DuckDB handles 5-10s read latency. Bottleneck is CPU.

- At 100GB scale: DuckDB handles 30-60s read latency. Bottleneck is I/O. Use streaming and caching.

- At 1TB+ scale: PostgreSQL handles 1-3 min read latency. Bottleneck is network and disk I/O. Use columnar storage and partitioning.

Capacity planning:
```
Memory (GB) = (Data size × 2) + 2GB overhead
Storage (GB) = Data size + (Data size × 0.3 for cache and intermediate results)
```

## Troubleshooting

You now have working pipelines that read from production sources, transform data with Xorq, write results with error handling, and track performance metrics. For troubleshooting common errors, see [Troubleshoot production data I/O](../../troubleshooting/data-io-errors.qmd).

## Next steps

- [Build data quality checks](build_data_quality_checks.qmd) - Validate data before processing
- [Transform data with custom UDFs](transform_data_with_custom_udfs.qmd) - Create reusable transformations
- [Optimize pipeline performance](../../guides/performance_workflows/optimize_pipeline_performance.qmd) - Improve I/O performance
- [Handle production errors](../../guides/platform_workflows/handle_production_errors.qmd) - Implement error handling
