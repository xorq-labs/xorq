import argparse
import logging
from datetime import datetime

import pandas as pd
import toolz

import xorq as xo
from xorq.common.utils.feature_utils import Entity, Feature, FeatureStore, FeatureView
from xorq.common.utils.import_utils import import_python
from xorq.flight import Backend as FlightBackend
from xorq.flight import FlightServer, FlightUrl


logging_format = "[%(asctime)s] %(levelname)s %(message)s"
logging.basicConfig(
    level=logging.INFO, format=logging_format, datefmt="%Y-%m-%d %H:%M:%S"
)

weather_lib = import_python("examples/libs/weather_lib.py")
do_fetch_current_weather_udxf = weather_lib.do_fetch_current_weather_udxf
do_fetch_current_weather_flight_udxf = weather_lib.do_fetch_current_weather_flight_udxf

WEATHER_FEATURES_PORT = weather_lib.WEATHER_FEATURES_PORT
TIMESTAMP_COLUMN = "timestamp"

# Database files
DB_BATCH = "weather_history_batch.db"  # full history batch store
DB_ONLINE = "weather_history.db"  # live UDXF ingestion store
TABLE_BATCH = "weather_history"
TABLE_ONLINE = "weather_history"
FEATURE_VIEW = "city_weather"
CITIES = ["London", "Tokyo", "New York", "Lahore"]


def setup_store() -> FeatureStore:
    logging.info("Setting up FeatureStore")

    # 1. Entity
    city = Entity("city", key_column="city", description="City identifier")

    # 2. Offline source (batch history)
    offline_con = xo.duckdb.connect()
    offline_con.raw_sql("""
        INSTALL ducklake;
        INSTALL sqlite;
        ATTACH 'ducklake:sqlite:metadata.sqlite' AS my_ducklake (DATA_PATH 'file_path/');
        USE my_ducklake;
        """)

    # 3. Flight backend for online features
    fb = FlightBackend()
    fb.do_connect(host="localhost", port=WEATHER_FEATURES_PORT)

    # 4. Build offline expression for features
    live_expr = xo.memtable(
        [{"city": c} for c in ["London", "Tokyo", "New York"]]
    ).pipe(do_fetch_current_weather_flight_udxf)
    win6_online = xo.window(
        group_by=[city.key_column], order_by=TIMESTAMP_COLUMN, preceding=5, following=0
    )

    # Offline expression that computes the feature
    offline_expr = live_expr.select(
        [
            city.key_column,
            TIMESTAMP_COLUMN,
            live_expr.temp_c.mean().over(win6_online).name("temp_mean_6h"),
        ]
    )

    # 5. Create Feature with only offline expression
    # Online expression will be auto-generated by FeatureStore
    feature_temp = Feature(
        name="temp_mean_6h",
        entity=city,
        timestamp_column=TIMESTAMP_COLUMN,
        offline_expr=offline_expr,
        description="6h rolling mean temp",
    )

    # 6. FeatureView & Store
    view = FeatureView(FEATURE_VIEW, city, [feature_temp])
    store = FeatureStore(online_client=fb.con)
    store.registry.register_entity(city)
    store.register_view(view)
    return store


def run_feature_server() -> None:
    server = FlightServer(
        FlightUrl(port=WEATHER_FEATURES_PORT),
        connection=xo.duckdb.connect,
    )
    logging.info(f"Serving feature store on grpc://localhost:{WEATHER_FEATURES_PORT}")

    def handle_keyboard_interrupt(_):
        logging.info("Keyboard Interrupt: Feature server shutting down")
        server.close()

    serve_excepting = toolz.excepts(
        KeyboardInterrupt, server.serve, handle_keyboard_interrupt
    )
    serve_excepting(block=True)


def run_materialize_online() -> None:
    store = setup_store()
    store.materialize_online(FEATURE_VIEW)
    logging.info("Materialized features to online store")


def run_infer() -> None:
    store = setup_store()
    df = store.get_online_features(FEATURE_VIEW, rows=[{"city": "London"}])
    logging.info("Retrieved online features")
    print(df)


def run_historical_features() -> None:
    """
    Demonstrate get_historical_features functionality similar to Feast
    """
    store = setup_store()

    # Create entity_df similar to Feast example
    entity_df = pd.DataFrame(
        {
            # Entity's join key -> entity values
            "city": ["London", "Tokyo", "New York"],
            # "event_timestamp" (reserved key) -> timestamps
            "event_timestamp": [
                datetime(2025, 6, 29, 23, 59, 42),
                datetime(2025, 6, 29, 23, 12, 10),
                datetime(2025, 6, 29, 23, 40, 26),
            ],
            # Optional label columns (not processed by feature store)
            "label_weather_satisfaction": [1, 5, 3],
            # Additional values for potential on-demand transformations
            "temp_adjustment": [1.0, 2.0, 3.0],
        }
    )

    training_df = store.get_historical_features(
        entity_df=entity_df,
        features=[
            f"{FEATURE_VIEW}:temp_mean_6h",
            # Add more features as needed:
            # f"{FEATURE_VIEW}:humidity_mean_6h",
            # f"{FEATURE_VIEW}:pressure_mean_6h",
        ],
    )

    logging.info("Retrieved historical features")
    print("Entity DataFrame:")
    print(entity_df)
    print("\nTraining DataFrame with Historical Features:")
    print(training_df)

    return training_df


def main() -> None:
    parser = argparse.ArgumentParser("Weather Flight Î£tore")
    parser.add_argument(
        "command",
        choices=(
            "serve_features",  # start feature lookup server
            "materialize_online",  # push latest to flight feature store
            "infer",
        ),
        help="Action: 'serve_features', 'materialize_offline', 'materialize_online', or 'infer'",
    )
    args = parser.parse_args()

    if args.command == "serve_features":
        run_feature_server()
    elif args.command == "materialize_online":
        run_materialize_online()
    elif args.command == "infer":
        run_infer()
    else:
        logging.error(f"Unknown command: {args.command}")


if __name__ == "__main__":
    main()
